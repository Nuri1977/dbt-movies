[0m10:03:08.064355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128ee400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113736c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113736dc0>]}


============================== 10:03:08.082232 | d642de27-4eec-4205-94dc-095016f39c22 ==============================
[0m10:03:08.082232 [info ] [MainThread]: Running with dbt=1.9.0
[0m10:03:08.084889 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:03:08.407665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd642de27-4eec-4205-94dc-095016f39c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136e4340>]}
[0m10:03:08.479090 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:03:08.482765 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:03:08.492795 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.6224087, "process_in_blocks": "0", "process_kernel_time": 0.588778, "process_mem_max_rss": "91250688", "process_out_blocks": "0", "process_user_time": 3.248886}
[0m10:03:08.493841 [debug] [MainThread]: Command `cli deps` succeeded at 10:03:08.493633 after 0.62 seconds
[0m10:03:08.494851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128ee400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11386f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136e4340>]}
[0m10:03:08.495632 [debug] [MainThread]: Flushing usage events
[0m10:03:09.303645 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:07:31.393970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ec8460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d10be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d10d90>]}


============================== 10:07:31.402960 | 18ffe395-5c9e-42b6-9f59-39ed87620e0b ==============================
[0m10:07:31.402960 [info ] [MainThread]: Running with dbt=1.9.0
[0m10:07:31.403922 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:07:31.584116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18ffe395-5c9e-42b6-9f59-39ed87620e0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b57760>]}
[0m10:07:31.632634 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:07:31.636919 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:07:31.648491 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.38295436, "process_in_blocks": "0", "process_kernel_time": 0.355148, "process_mem_max_rss": "90968064", "process_out_blocks": "0", "process_user_time": 2.000781}
[0m10:07:31.649328 [debug] [MainThread]: Command `cli deps` succeeded at 10:07:31.649151 after 0.38 seconds
[0m10:07:31.649928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ec8460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e49490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b57760>]}
[0m10:07:31.650549 [debug] [MainThread]: Flushing usage events
[0m10:07:32.388684 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:45:07.218377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10777e370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085bfb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085bfd00>]}


============================== 13:45:07.224342 | 60e482d2-9bd8-478c-b39d-745ba9369658 ==============================
[0m13:45:07.224342 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:45:07.225381 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:45:07.368817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '60e482d2-9bd8-478c-b39d-745ba9369658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085bf1f0>]}
[0m13:45:07.386449 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:45:07.387860 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:45:07.392008 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.25528446, "process_in_blocks": "0", "process_kernel_time": 0.179321, "process_mem_max_rss": "90980352", "process_out_blocks": "0", "process_user_time": 1.71518}
[0m13:45:07.392594 [debug] [MainThread]: Command `cli deps` succeeded at 13:45:07.392465 after 0.26 seconds
[0m13:45:07.393037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10777e370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087003d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085bf1f0>]}
[0m13:45:07.393494 [debug] [MainThread]: Flushing usage events
[0m13:45:08.080952 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:19.158712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047a7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055ee4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055ee310>]}


============================== 10:49:19.169654 | f87d6e2d-e584-4f24-8c7d-ecb2d1826bc3 ==============================
[0m10:49:19.169654 [info ] [MainThread]: Running with dbt=1.9.0
[0m10:49:19.171554 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m10:49:19.496464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f87d6e2d-e584-4f24-8c7d-ecb2d1826bc3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055eee20>]}
[0m10:49:19.719814 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:49:19.724655 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:49:19.758614 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.8763698, "process_in_blocks": "0", "process_kernel_time": 0.639049, "process_mem_max_rss": "91332608", "process_out_blocks": "0", "process_user_time": 3.616328}
[0m10:49:19.761912 [debug] [MainThread]: Command `cli deps` succeeded at 10:49:19.761546 after 0.88 seconds
[0m10:49:19.763290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047a7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10572a2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055eee20>]}
[0m10:49:19.764576 [debug] [MainThread]: Flushing usage events
[0m10:49:20.794015 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:45:23.404419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aaab400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8f3c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8f3dc0>]}


============================== 09:45:23.417548 | 7d04e86a-3739-46a0-81f8-5299dad493cf ==============================
[0m09:45:23.417548 [info ] [MainThread]: Running with dbt=1.9.0
[0m09:45:23.418561 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:45:23.657047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7d04e86a-3739-46a0-81f8-5299dad493cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8f32b0>]}
[0m09:45:23.709930 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:45:23.711781 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:45:23.723177 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.46037695, "process_in_blocks": "0", "process_kernel_time": 0.317231, "process_mem_max_rss": "91389952", "process_out_blocks": "0", "process_user_time": 1.898078}
[0m09:45:23.723969 [debug] [MainThread]: Command `cli deps` succeeded at 09:45:23.723799 after 0.46 seconds
[0m09:45:23.724534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aaab400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba2c460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8f32b0>]}
[0m09:45:23.725128 [debug] [MainThread]: Flushing usage events
[0m09:45:24.882918 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:57:26.251397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eefe400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd47b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd47d30>]}


============================== 09:57:26.259734 | 58857019-ffea-4847-997e-aba027ada07f ==============================
[0m09:57:26.259734 [info ] [MainThread]: Running with dbt=1.9.0
[0m09:57:26.260643 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'version_check': 'True', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:57:26.430942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '58857019-ffea-4847-997e-aba027ada07f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd94430>]}
[0m09:57:26.474043 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:57:26.476114 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:57:26.482087 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.34067565, "process_in_blocks": "0", "process_kernel_time": 0.379386, "process_mem_max_rss": "90984448", "process_out_blocks": "0", "process_user_time": 2.080811}
[0m09:57:26.482839 [debug] [MainThread]: Command `cli deps` succeeded at 09:57:26.482674 after 0.34 seconds
[0m09:57:26.483398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eefe400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe7b430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd94430>]}
[0m09:57:26.483983 [debug] [MainThread]: Flushing usage events
[0m09:57:27.493434 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:58:12.456942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be8b430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d189b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d189d00>]}


============================== 09:58:12.471620 | a3f5a6b2-d399-40ff-b3e3-c9eef01cc3a8 ==============================
[0m09:58:12.471620 [info ] [MainThread]: Running with dbt=1.9.0
[0m09:58:12.472594 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m09:58:12.660874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a3f5a6b2-d399-40ff-b3e3-c9eef01cc3a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1e21f0>]}
[0m09:58:12.730312 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:58:12.732253 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:58:12.744376 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.58432925, "process_in_blocks": "0", "process_kernel_time": 0.413091, "process_mem_max_rss": "91242496", "process_out_blocks": "0", "process_user_time": 2.094238}
[0m09:58:12.745181 [debug] [MainThread]: Command `cli deps` succeeded at 09:58:12.745008 after 0.59 seconds
[0m09:58:12.745771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be8b430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2c1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1e21f0>]}
[0m09:58:12.746387 [debug] [MainThread]: Flushing usage events
[0m09:58:13.465127 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:20:17.665037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112df2400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c3bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c3bdc0>]}


============================== 20:20:17.734593 | 923f2df2-47b6-402e-ab79-8134d8151acd ==============================
[0m20:20:17.734593 [info ] [MainThread]: Running with dbt=1.9.0
[0m20:20:17.735465 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:20:17.901153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '923f2df2-47b6-402e-ab79-8134d8151acd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113be7340>]}
[0m20:20:18.101138 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m20:20:18.103062 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m20:20:18.156762 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.6158079, "process_in_blocks": "0", "process_kernel_time": 0.353989, "process_mem_max_rss": "91447296", "process_out_blocks": "0", "process_user_time": 2.027213}
[0m20:20:18.157663 [debug] [MainThread]: Command `cli deps` succeeded at 20:20:18.157360 after 0.62 seconds
[0m20:20:18.158318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112df2400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d74460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113be7340>]}
[0m20:20:18.158907 [debug] [MainThread]: Flushing usage events
[0m20:20:18.725589 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:49:26.964969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122bd4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1135bbca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1135bbe50>]}


============================== 13:49:26.974340 | 1b86dfdd-a56f-4027-9f56-9818cd167583 ==============================
[0m13:49:26.974340 [info ] [MainThread]: Running with dbt=1.9.0
[0m13:49:26.975334 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:49:27.153267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1b86dfdd-a56f-4027-9f56-9818cd167583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113572ca0>]}
[0m13:49:27.217398 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:49:27.219532 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:49:27.230561 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.40143776, "process_in_blocks": "0", "process_kernel_time": 0.382409, "process_mem_max_rss": "91250688", "process_out_blocks": "0", "process_user_time": 2.078792}
[0m13:49:27.231743 [debug] [MainThread]: Command `cli deps` succeeded at 13:49:27.231544 after 0.40 seconds
[0m13:49:27.232387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122bd4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136f3760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11370ef40>]}
[0m13:49:27.233051 [debug] [MainThread]: Flushing usage events
[0m13:49:34.019058 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:29:36.414933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10982f430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab2cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab2cd00>]}


============================== 14:29:36.422339 | 1a82b527-48f6-4aae-b130-12173b8e7541 ==============================
[0m14:29:36.422339 [info ] [MainThread]: Running with dbt=1.9.0
[0m14:29:36.423611 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:29:36.633293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a82b527-48f6-4aae-b130-12173b8e7541', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abccca0>]}
[0m14:29:36.676401 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m14:29:36.678485 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m14:29:36.683713 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.3833262, "process_in_blocks": "0", "process_kernel_time": 0.356858, "process_mem_max_rss": "91189248", "process_out_blocks": "0", "process_user_time": 1.99986}
[0m14:29:36.684497 [debug] [MainThread]: Command `cli deps` succeeded at 14:29:36.684325 after 0.38 seconds
[0m14:29:36.685065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10982f430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac5f490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abccca0>]}
[0m14:29:36.685667 [debug] [MainThread]: Flushing usage events
[0m14:29:37.290470 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:37:10.279817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103699430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104997b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104997d00>]}


============================== 18:37:10.309078 | 1d61f7c0-8c6e-4907-ba1f-2999874d5e48 ==============================
[0m18:37:10.309078 [info ] [MainThread]: Running with dbt=1.9.0
[0m18:37:10.310432 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:37:10.885451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1d61f7c0-8c6e-4907-ba1f-2999874d5e48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049f01f0>]}
[0m18:37:11.093381 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m18:37:11.098254 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m18:37:11.145578 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.2263154, "process_in_blocks": "0", "process_kernel_time": 0.990348, "process_mem_max_rss": "91127808", "process_out_blocks": "0", "process_user_time": 4.841687}
[0m18:37:11.147184 [debug] [MainThread]: Command `cli deps` succeeded at 18:37:11.146827 after 1.23 seconds
[0m18:37:11.152856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103699430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ad0490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049f01f0>]}
[0m18:37:11.160159 [debug] [MainThread]: Flushing usage events
[0m18:37:12.225863 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:16:13.720092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a6e3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b6be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088b6d90>]}


============================== 19:16:13.733506 | 25ede15f-22e0-4c6e-990f-20eef2432a31 ==============================
[0m19:16:13.733506 [info ] [MainThread]: Running with dbt=1.9.0
[0m19:16:13.734594 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m19:16:13.891665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25ede15f-22e0-4c6e-990f-20eef2432a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108875e20>]}
[0m19:16:14.013472 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m19:16:14.016348 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m19:16:14.033511 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.48741555, "process_in_blocks": "0", "process_kernel_time": 0.477997, "process_mem_max_rss": "91643904", "process_out_blocks": "0", "process_user_time": 2.409608}
[0m19:16:14.034339 [debug] [MainThread]: Command `cli deps` succeeded at 19:16:14.034168 after 0.49 seconds
[0m19:16:14.034936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a6e3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089ea400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108875e20>]}
[0m19:16:14.035580 [debug] [MainThread]: Flushing usage events
[0m19:16:14.833847 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:28:39.554224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108146460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f8fc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f8fe20>]}


============================== 19:28:39.562657 | 69b9ce08-c61d-4895-9407-f581666d12f3 ==============================
[0m19:28:39.562657 [info ] [MainThread]: Running with dbt=1.9.0
[0m19:28:39.563505 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample', 'log_path': '/Users/nuri/Documents/Projects/dbt-studio/assets/dbt_sample/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:28:39.756562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69b9ce08-c61d-4895-9407-f581666d12f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f79220>]}
[0m19:28:39.793181 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m19:28:39.794930 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m19:28:39.799257 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.38392895, "process_in_blocks": "0", "process_kernel_time": 0.365211, "process_mem_max_rss": "91283456", "process_out_blocks": "0", "process_user_time": 1.911112}
[0m19:28:39.799937 [debug] [MainThread]: Command `cli deps` succeeded at 19:28:39.799795 after 0.38 seconds
[0m19:28:39.800561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108146460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090c34c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f79220>]}
[0m19:28:39.801061 [debug] [MainThread]: Flushing usage events
[0m19:28:40.607552 [debug] [MainThread]: An error was encountered while trying to flush usage events


============================== 02:02:55.592065 | 77687b14-5eaa-4e2e-a071-9ce1c499e340 ==============================
[0m02:02:55.592065 [info ] [MainThread]: Running with dbt=1.10.11
[0m02:02:55.603008 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'no_print': 'None', 'target_path': 'None', 'empty': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'write_json': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'debug': 'False', 'send_anonymous_usage_stats': 'False', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'profiles_dir': '/Users/nuri/rosetta-dbt-studio-projects/movies', 'log_path': '/Users/nuri/rosetta-dbt-studio-projects/movies/logs', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m02:02:55.603749 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m02:02:59.337198 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:02:59.337980 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:02:59.338554 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:03:26.113442 [info ] [MainThread]: Registered adapter: databricks=1.10.12
[0m02:03:26.386250 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m02:03:26.390142 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m02:03:29.237530 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:03:29.243036 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:03:29.296806 [info ] [MainThread]: Found 7 sources, 686 macros
[0m02:03:29.299114 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m02:03:29.300131 [debug] [MainThread]: Command end result
[0m02:03:29.418369 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:03:29.424931 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:03:29.430417 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nuri/rosetta-dbt-studio-projects/movies/target/run_results.json
[0m02:03:29.431269 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ProjectFlagsMovedDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:03:29.432661 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 34.010845, "process_in_blocks": "0", "process_kernel_time": 1.125238, "process_mem_max_rss": "206807040", "process_out_blocks": "0", "process_user_time": 6.307899}
[0m02:03:29.433371 [debug] [MainThread]: Command `dbt run` succeeded at 02:03:29.433225 after 34.01 seconds
[0m02:03:29.433995 [debug] [MainThread]: Flushing usage events


============================== 02:04:52.390801 | a5f72b00-0973-4153-8514-00773a008c82 ==============================
[0m02:04:52.390801 [info ] [MainThread]: Running with dbt=1.10.11
[0m02:04:52.399622 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'no_print': 'None', 'empty': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'log_format': 'default', 'write_json': 'True', 'cache_selected_only': 'False', 'use_colors': 'True', 'target_path': 'None', 'log_path': '/Users/nuri/rosetta-dbt-studio-projects/movies/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'version_check': 'True', 'partial_parse': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'False', 'invocation_command': 'dbt run', 'debug': 'False', 'profiles_dir': '/Users/nuri/rosetta-dbt-studio-projects/movies', 'indirect_selection': 'eager', 'use_experimental_parser': 'False'}
[0m02:04:52.400305 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m02:04:54.411413 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:04:54.412075 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:04:54.412614 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:04:57.141213 [info ] [MainThread]: Registered adapter: databricks=1.10.12
[0m02:04:57.445814 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m02:04:57.967876 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:04:57.968499 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:04:58.107991 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:04:58.114621 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:04:58.159249 [info ] [MainThread]: Found 7 sources, 686 macros
[0m02:04:58.161544 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m02:04:58.164456 [debug] [MainThread]: Command end result
[0m02:04:58.280383 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:04:58.286318 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:04:58.294092 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nuri/rosetta-dbt-studio-projects/movies/target/run_results.json
[0m02:04:58.295049 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ProjectFlagsMovedDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:04:58.296178 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.036301, "process_in_blocks": "0", "process_kernel_time": 0.950431, "process_mem_max_rss": "201474048", "process_out_blocks": "0", "process_user_time": 4.101613}
[0m02:04:58.297008 [debug] [MainThread]: Command `dbt run` succeeded at 02:04:58.296861 after 6.04 seconds
[0m02:04:58.297632 [debug] [MainThread]: Flushing usage events


============================== 02:06:22.956896 | da4dd95c-d806-44a5-b686-50b3c605bf26 ==============================
[0m02:06:22.956896 [info ] [MainThread]: Running with dbt=1.10.11
[0m02:06:22.964830 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'log_path': '/Users/nuri/rosetta-dbt-studio-projects/movies/logs', 'no_print': 'None', 'write_json': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'send_anonymous_usage_stats': 'False', 'profiles_dir': '/Users/nuri/rosetta-dbt-studio-projects/movies', 'target_path': 'None', 'static_parser': 'True', 'quiet': 'False', 'debug': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'introspect': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'printer_width': '80', 'use_experimental_parser': 'False'}
[0m02:06:22.965741 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m02:06:24.337481 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:06:24.338640 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:06:24.340274 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:06:25.899983 [info ] [MainThread]: Registered adapter: databricks=1.10.12
[0m02:06:26.119016 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m02:06:26.497153 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m02:06:26.497742 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_movies_metadata.sql
[0m02:06:26.498152 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_links_small.sql
[0m02:06:26.498692 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_links.sql
[0m02:06:26.499073 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_ratings_small.sql
[0m02:06:26.499495 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_keywords.sql
[0m02:06:26.499876 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_ratings.sql
[0m02:06:26.500256 [debug] [MainThread]: Partial parsing: added file: movies://models/staging/default_credits.sql
[0m02:06:26.964423 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:06:26.969265 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:06:26.991631 [info ] [MainThread]: Found 7 models, 7 sources, 686 macros
[0m02:06:26.994199 [info ] [MainThread]: 
[0m02:06:26.994840 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m02:06:26.995371 [info ] [MainThread]: 
[0m02:06:26.996170 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:06:26.996769 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:06:27.008519 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_movies) - Creating connection
[0m02:06:27.009469 [debug] [ThreadPool]: Acquiring new databricks connection 'list_movies'
[0m02:06:27.122042 [debug] [ThreadPool]: Using databricks connection "list_movies"
[0m02:06:27.123206 [debug] [ThreadPool]: On list_movies: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "connection_name": "list_movies"} */

    show databases
  
[0m02:06:27.124028 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:06:27.921637 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-4bd4-11c9-b55e-5124cfcaffe7) - Created
[0m02:06:28.489936 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m02:06:28.515061 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0cfe4-4bd4-11c9-b55e-5124cfcaffe7, command-id=01f0cfe4-4bf0-1aa4-889b-75004f74ab5b) - Closing
[0m02:06:28.515965 [debug] [ThreadPool]: On list_movies: Close
[0m02:06:28.516492 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-4bd4-11c9-b55e-5124cfcaffe7) - Closing
[0m02:06:28.708047 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_movies_default) - Creating connection
[0m02:06:28.708736 [debug] [ThreadPool]: Acquiring new databricks connection 'list_movies_default'
[0m02:06:28.721154 [debug] [ThreadPool]: Using databricks connection "list_movies_default"
[0m02:06:28.721791 [debug] [ThreadPool]: On list_movies_default: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "connection_name": "list_movies_default"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'movies' 
  AND table_schema = 'default'

  
[0m02:06:28.722286 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:06:29.441732 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-4cbb-1422-9069-cdda9f113a84) - Created
[0m02:06:30.178878 [debug] [ThreadPool]: SQL status: OK in 1.460 seconds
[0m02:06:30.183176 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0cfe4-4cbb-1422-9069-cdda9f113a84, command-id=01f0cfe4-4cd8-15d2-b59f-1f9ab1ddb2b3) - Closing
[0m02:06:30.184180 [debug] [ThreadPool]: On list_movies_default: Close
[0m02:06:30.184705 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-4cbb-1422-9069-cdda9f113a84) - Closing
[0m02:06:30.406612 [debug] [Thread-1 (]: Began running node model.movies.default_credits
[0m02:06:30.407246 [debug] [Thread-2 (]: Began running node model.movies.default_keywords
[0m02:06:30.407904 [debug] [Thread-3 (]: Began running node model.movies.default_links
[0m02:06:30.408633 [info ] [Thread-1 (]: 1 of 7 START sql view model default.default_credits ............................ [RUN]
[0m02:06:30.409331 [debug] [Thread-4 (]: Began running node model.movies.default_links_small
[0m02:06:30.410029 [info ] [Thread-2 (]: 2 of 7 START sql view model default.default_keywords ........................... [RUN]
[0m02:06:30.410693 [info ] [Thread-3 (]: 3 of 7 START sql view model default.default_links .............................. [RUN]
[0m02:06:30.411732 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_credits) - Creating connection
[0m02:06:30.412476 [info ] [Thread-4 (]: 4 of 7 START sql view model default.default_links_small ........................ [RUN]
[0m02:06:30.413293 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_keywords) - Creating connection
[0m02:06:30.414072 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_links) - Creating connection
[0m02:06:30.414691 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.movies.default_credits'
[0m02:06:30.415547 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_links_small) - Creating connection
[0m02:06:30.416173 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.movies.default_keywords'
[0m02:06:30.416739 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.movies.default_links'
[0m02:06:30.417337 [debug] [Thread-1 (]: Began compiling node model.movies.default_credits
[0m02:06:30.417912 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.movies.default_links_small'
[0m02:06:30.418480 [debug] [Thread-2 (]: Began compiling node model.movies.default_keywords
[0m02:06:30.419081 [debug] [Thread-3 (]: Began compiling node model.movies.default_links
[0m02:06:30.430628 [debug] [Thread-1 (]: Writing injected SQL for node "model.movies.default_credits"
[0m02:06:30.431504 [debug] [Thread-4 (]: Began compiling node model.movies.default_links_small
[0m02:06:30.436473 [debug] [Thread-2 (]: Writing injected SQL for node "model.movies.default_keywords"
[0m02:06:30.441957 [debug] [Thread-3 (]: Writing injected SQL for node "model.movies.default_links"
[0m02:06:30.448402 [debug] [Thread-4 (]: Writing injected SQL for node "model.movies.default_links_small"
[0m02:06:30.453523 [debug] [Thread-2 (]: Began executing node model.movies.default_keywords
[0m02:06:30.460018 [debug] [Thread-3 (]: Began executing node model.movies.default_links
[0m02:06:30.460658 [debug] [Thread-1 (]: Began executing node model.movies.default_credits
[0m02:06:30.478147 [debug] [Thread-4 (]: Began executing node model.movies.default_links_small
[0m02:06:30.483995 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:06:30.484676 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m02:06:30.488093 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m02:06:30.491503 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:06:30.494394 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:06:30.495314 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:06:30.496199 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:06:30.497141 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:06:30.515752 [debug] [Thread-3 (]: Creating view `movies`.`default`.`default_links`
[0m02:06:30.517314 [debug] [Thread-2 (]: Creating view `movies`.`default`.`default_keywords`
[0m02:06:30.518807 [debug] [Thread-1 (]: Creating view `movies`.`default`.`default_credits`
[0m02:06:30.520216 [debug] [Thread-4 (]: Creating view `movies`.`default`.`default_links_small`
[0m02:06:30.530814 [debug] [Thread-3 (]: Writing runtime sql for node "model.movies.default_links"
[0m02:06:30.532593 [debug] [Thread-2 (]: Writing runtime sql for node "model.movies.default_keywords"
[0m02:06:30.534199 [debug] [Thread-1 (]: Writing runtime sql for node "model.movies.default_credits"
[0m02:06:30.535563 [debug] [Thread-4 (]: Writing runtime sql for node "model.movies.default_links_small"
[0m02:06:30.542337 [debug] [Thread-1 (]: Using databricks connection "model.movies.default_credits"
[0m02:06:30.543188 [debug] [Thread-1 (]: On model.movies.default_credits: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_credits"} */

  
  
  create or replace view `movies`.`default`.`default_credits`
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  )

[0m02:06:30.544000 [debug] [Thread-2 (]: Using databricks connection "model.movies.default_keywords"
[0m02:06:30.544455 [debug] [Thread-4 (]: Using databricks connection "model.movies.default_links_small"
[0m02:06:30.544929 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:06:30.545342 [debug] [Thread-3 (]: Using databricks connection "model.movies.default_links"
[0m02:06:30.545903 [debug] [Thread-2 (]: On model.movies.default_keywords: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_keywords"} */

  
  
  create or replace view `movies`.`default`.`default_keywords`
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  )

[0m02:06:30.546472 [debug] [Thread-4 (]: On model.movies.default_links_small: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_links_small"} */

  
  
  create or replace view `movies`.`default`.`default_links_small`
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  )

[0m02:06:30.547435 [debug] [Thread-3 (]: On model.movies.default_links: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_links"} */

  
  
  create or replace view `movies`.`default`.`default_links`
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  )

[0m02:06:30.548564 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m02:06:30.549754 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:06:30.550492 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:06:31.411285 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-4de9-1a8d-bb58-46b331d037b4) - Created
[0m02:06:31.423728 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-4dea-1204-97e8-3fe6acf179d3) - Created
[0m02:06:31.425338 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-4dea-1030-99cc-00992af22c2d) - Created
[0m02:06:31.426655 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-4dea-1271-b6df-b16e54df977a) - Created
[0m02:06:32.220729 [debug] [Thread-4 (]: SQL status: OK in 1.670 seconds
[0m02:06:32.223621 [debug] [Thread-1 (]: SQL status: OK in 1.680 seconds
[0m02:06:32.225381 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4dea-1271-b6df-b16e54df977a, command-id=01f0cfe4-4e07-1579-90cb-6c2cf0f33b9a) - Closing
[0m02:06:32.226702 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4de9-1a8d-bb58-46b331d037b4, command-id=01f0cfe4-4e04-1703-966e-0fc5551cbc23) - Closing
[0m02:06:32.243054 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:06:32.246283 [debug] [Thread-2 (]: SQL status: OK in 1.700 seconds
[0m02:06:32.247497 [debug] [Thread-1 (]: Applying tags to relation None
[0m02:06:32.252327 [debug] [Thread-4 (]: On model.movies.default_links_small: Close
[0m02:06:32.253654 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4dea-1204-97e8-3fe6acf179d3, command-id=01f0cfe4-4e06-1ff9-a2f4-f61f5ecb417a) - Closing
[0m02:06:32.255050 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-4dea-1271-b6df-b16e54df977a) - Closing
[0m02:06:32.256833 [debug] [Thread-2 (]: Applying tags to relation None
[0m02:06:32.287181 [debug] [Thread-3 (]: SQL status: OK in 1.740 seconds
[0m02:06:32.288509 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4dea-1030-99cc-00992af22c2d, command-id=01f0cfe4-4e09-1a5e-879f-43d03ee068c6) - Closing
[0m02:06:32.289717 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:06:32.463170 [debug] [Thread-1 (]: On model.movies.default_credits: Close
[0m02:06:32.463992 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-4de9-1a8d-bb58-46b331d037b4) - Closing
[0m02:06:32.664059 [debug] [Thread-2 (]: On model.movies.default_keywords: Close
[0m02:06:32.664795 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-4dea-1204-97e8-3fe6acf179d3) - Closing
[0m02:06:32.862028 [debug] [Thread-3 (]: On model.movies.default_links: Close
[0m02:06:32.862873 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-4dea-1030-99cc-00992af22c2d) - Closing
[0m02:06:33.073925 [info ] [Thread-4 (]: 4 of 7 OK created sql view model default.default_links_small ................... [[32mOK[0m in 2.65s]
[0m02:06:33.074670 [info ] [Thread-1 (]: 1 of 7 OK created sql view model default.default_credits ....................... [[32mOK[0m in 2.66s]
[0m02:06:33.075504 [info ] [Thread-3 (]: 3 of 7 OK created sql view model default.default_links ......................... [[32mOK[0m in 2.66s]
[0m02:06:33.077308 [debug] [Thread-4 (]: Finished running node model.movies.default_links_small
[0m02:06:33.076185 [info ] [Thread-2 (]: 2 of 7 OK created sql view model default.default_keywords ...................... [[32mOK[0m in 2.66s]
[0m02:06:33.078269 [debug] [Thread-1 (]: Finished running node model.movies.default_credits
[0m02:06:33.079071 [debug] [Thread-3 (]: Finished running node model.movies.default_links
[0m02:06:33.079722 [debug] [Thread-4 (]: Began running node model.movies.default_movies_metadata
[0m02:06:33.080629 [debug] [Thread-2 (]: Finished running node model.movies.default_keywords
[0m02:06:33.081276 [debug] [Thread-1 (]: Began running node model.movies.default_ratings
[0m02:06:33.081964 [debug] [Thread-3 (]: Began running node model.movies.default_ratings_small
[0m02:06:33.082853 [info ] [Thread-4 (]: 5 of 7 START sql view model default.default_movies_metadata .................... [RUN]
[0m02:06:33.083828 [info ] [Thread-1 (]: 6 of 7 START sql view model default.default_ratings ............................ [RUN]
[0m02:06:33.084614 [info ] [Thread-3 (]: 7 of 7 START sql view model default.default_ratings_small ...................... [RUN]
[0m02:06:33.085491 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_movies_metadata) - Creating connection
[0m02:06:33.086264 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_ratings) - Creating connection
[0m02:06:33.087027 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_ratings_small) - Creating connection
[0m02:06:33.087613 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.movies.default_movies_metadata'
[0m02:06:33.088187 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.movies.default_ratings'
[0m02:06:33.088761 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.movies.default_ratings_small'
[0m02:06:33.089393 [debug] [Thread-4 (]: Began compiling node model.movies.default_movies_metadata
[0m02:06:33.089989 [debug] [Thread-1 (]: Began compiling node model.movies.default_ratings
[0m02:06:33.090574 [debug] [Thread-3 (]: Began compiling node model.movies.default_ratings_small
[0m02:06:33.095329 [debug] [Thread-4 (]: Writing injected SQL for node "model.movies.default_movies_metadata"
[0m02:06:33.100003 [debug] [Thread-1 (]: Writing injected SQL for node "model.movies.default_ratings"
[0m02:06:33.104323 [debug] [Thread-3 (]: Writing injected SQL for node "model.movies.default_ratings_small"
[0m02:06:33.106342 [debug] [Thread-4 (]: Began executing node model.movies.default_movies_metadata
[0m02:06:33.110150 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:06:33.110737 [debug] [Thread-1 (]: Began executing node model.movies.default_ratings
[0m02:06:33.111395 [debug] [Thread-3 (]: Began executing node model.movies.default_ratings_small
[0m02:06:33.113341 [debug] [Thread-4 (]: Creating view `movies`.`default`.`default_movies_metadata`
[0m02:06:33.119678 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:06:33.123472 [debug] [Thread-4 (]: Writing runtime sql for node "model.movies.default_movies_metadata"
[0m02:06:33.126442 [debug] [Thread-3 (]: Creating view `movies`.`default`.`default_ratings_small`
[0m02:06:33.135075 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m02:06:33.136821 [debug] [Thread-3 (]: Writing runtime sql for node "model.movies.default_ratings_small"
[0m02:06:33.138730 [debug] [Thread-1 (]: Creating view `movies`.`default`.`default_ratings`
[0m02:06:33.139385 [debug] [Thread-4 (]: Using databricks connection "model.movies.default_movies_metadata"
[0m02:06:33.140960 [debug] [Thread-1 (]: Writing runtime sql for node "model.movies.default_ratings"
[0m02:06:33.141646 [debug] [Thread-4 (]: On model.movies.default_movies_metadata: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_movies_metadata"} */

  
  
  create or replace view `movies`.`default`.`default_movies_metadata`
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  )

[0m02:06:33.142413 [debug] [Thread-3 (]: Using databricks connection "model.movies.default_ratings_small"
[0m02:06:33.143030 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:06:33.143705 [debug] [Thread-3 (]: On model.movies.default_ratings_small: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_ratings_small"} */

  
  
  create or replace view `movies`.`default`.`default_ratings_small`
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  )

[0m02:06:33.144572 [debug] [Thread-1 (]: Using databricks connection "model.movies.default_ratings"
[0m02:06:33.147681 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:06:33.148712 [debug] [Thread-1 (]: On model.movies.default_ratings: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_ratings"} */

  
  
  create or replace view `movies`.`default`.`default_ratings`
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  )

[0m02:06:33.149778 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:06:33.920504 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-4f68-1229-b0dd-28b9e9454bfc) - Created
[0m02:06:33.926299 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-4f69-10bd-99e7-430ac6e3e302) - Created
[0m02:06:33.936929 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-4f6a-1c14-a8e9-b9bd46506ad6) - Created
[0m02:06:34.687364 [debug] [Thread-3 (]: SQL status: OK in 1.540 seconds
[0m02:06:34.690510 [debug] [Thread-4 (]: SQL status: OK in 1.550 seconds
[0m02:06:34.691784 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4f68-1229-b0dd-28b9e9454bfc, command-id=01f0cfe4-4f82-1f2e-b810-8c08e26c14d7) - Closing
[0m02:06:34.694245 [debug] [Thread-1 (]: SQL status: OK in 1.540 seconds
[0m02:06:34.695415 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4f69-10bd-99e7-430ac6e3e302, command-id=01f0cfe4-4f84-1224-a75f-c1fc10129119) - Closing
[0m02:06:34.696552 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:06:34.697718 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0cfe4-4f6a-1c14-a8e9-b9bd46506ad6, command-id=01f0cfe4-4f85-1ce2-94ed-574b4a535820) - Closing
[0m02:06:34.698971 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:06:34.700122 [debug] [Thread-3 (]: On model.movies.default_ratings_small: Close
[0m02:06:34.701185 [debug] [Thread-1 (]: Applying tags to relation None
[0m02:06:34.702357 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-4f68-1229-b0dd-28b9e9454bfc) - Closing
[0m02:06:34.904818 [debug] [Thread-4 (]: On model.movies.default_movies_metadata: Close
[0m02:06:34.905837 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-4f69-10bd-99e7-430ac6e3e302) - Closing
[0m02:06:35.100548 [debug] [Thread-1 (]: On model.movies.default_ratings: Close
[0m02:06:35.101498 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-4f6a-1c14-a8e9-b9bd46506ad6) - Closing
[0m02:06:35.303846 [info ] [Thread-3 (]: 7 of 7 OK created sql view model default.default_ratings_small ................. [[32mOK[0m in 2.22s]
[0m02:06:35.304819 [info ] [Thread-4 (]: 5 of 7 OK created sql view model default.default_movies_metadata ............... [[32mOK[0m in 2.22s]
[0m02:06:35.306007 [info ] [Thread-1 (]: 6 of 7 OK created sql view model default.default_ratings ....................... [[32mOK[0m in 2.22s]
[0m02:06:35.307570 [debug] [Thread-3 (]: Finished running node model.movies.default_ratings_small
[0m02:06:35.308391 [debug] [Thread-4 (]: Finished running node model.movies.default_movies_metadata
[0m02:06:35.309169 [debug] [Thread-1 (]: Finished running node model.movies.default_ratings
[0m02:06:35.310981 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:06:35.311514 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:06:35.312221 [info ] [MainThread]: 
[0m02:06:35.312775 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 8.32 seconds (8.32s).
[0m02:06:35.314918 [debug] [MainThread]: Command end result
[0m02:06:35.429400 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:06:35.434918 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:06:35.444618 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nuri/rosetta-dbt-studio-projects/movies/target/run_results.json
[0m02:06:35.445184 [info ] [MainThread]: 
[0m02:06:35.445766 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:06:35.446265 [info ] [MainThread]: 
[0m02:06:35.446791 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m02:06:35.447707 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ProjectFlagsMovedDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:06:35.448604 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.601521, "process_in_blocks": "0", "process_kernel_time": 0.869303, "process_mem_max_rss": "219623424", "process_out_blocks": "0", "process_user_time": 5.947796}
[0m02:06:35.449284 [debug] [MainThread]: Command `dbt run` succeeded at 02:06:35.449137 after 12.60 seconds
[0m02:06:35.450166 [debug] [MainThread]: Flushing usage events


============================== 02:07:52.017379 | 052e3f53-2d7e-4383-ba4c-1c39fa394f1e ==============================
[0m02:07:52.017379 [info ] [MainThread]: Running with dbt=1.10.11
[0m02:07:52.025673 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'log_path': '/Users/nuri/rosetta-dbt-studio-projects/movies/logs', 'introspect': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/Users/nuri/rosetta-dbt-studio-projects/movies', 'warn_error': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'False', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'version_check': 'True', 'target_path': 'None', 'empty': 'False', 'fail_fast': 'False', 'log_format': 'default', 'partial_parse': 'True', 'quiet': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'use_colors': 'True', 'write_json': 'True'}
[0m02:07:52.026431 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality

User config should be moved from the 'config' key in profiles.yml to the 'flags' key in dbt_project.yml.
[0m02:07:53.421327 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:07:53.421924 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:07:53.422336 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:07:55.226219 [info ] [MainThread]: Registered adapter: databricks=1.10.12
[0m02:07:55.434137 [debug] [MainThread]: checksum: 5335e1629744fab92fa7809c625edbb36f0d7063a879d6215c971798f27e66f9, vars: {}, profile: , target: , version: 1.10.11
[0m02:07:55.868603 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m02:07:55.869338 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_ratings.sql
[0m02:07:55.869777 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_links.sql
[0m02:07:55.870175 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_links_small.sql
[0m02:07:55.870580 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_ratings_small.sql
[0m02:07:55.870980 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_movies_metadata.sql
[0m02:07:55.871375 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_credits.sql
[0m02:07:55.871773 [debug] [MainThread]: Partial parsing: added file: movies://models/enhanced/enh_default_keywords.sql
[0m02:07:56.358830 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:07:56.363600 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:07:56.391378 [info ] [MainThread]: Found 14 models, 7 sources, 686 macros
[0m02:07:56.394413 [info ] [MainThread]: 
[0m02:07:56.395043 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m02:07:56.395515 [info ] [MainThread]: 
[0m02:07:56.396293 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:07:56.396837 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:07:56.409544 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_movies) - Creating connection
[0m02:07:56.410238 [debug] [ThreadPool]: Acquiring new databricks connection 'list_movies'
[0m02:07:56.424516 [debug] [ThreadPool]: Using databricks connection "list_movies"
[0m02:07:56.425135 [debug] [ThreadPool]: On list_movies: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "connection_name": "list_movies"} */

    show databases
  
[0m02:07:56.425623 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:07:57.154542 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-8104-182e-b12a-c8b051bd4a5a) - Created
[0m02:07:57.634563 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m02:07:57.647011 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0cfe4-8104-182e-b12a-c8b051bd4a5a, command-id=01f0cfe4-8120-12af-b92e-36c5736878b5) - Closing
[0m02:07:57.647821 [debug] [ThreadPool]: On list_movies: Close
[0m02:07:57.648347 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-8104-182e-b12a-c8b051bd4a5a) - Closing
[0m02:07:57.852197 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_movies_default) - Creating connection
[0m02:07:57.853120 [debug] [ThreadPool]: Acquiring new databricks connection 'list_movies_default'
[0m02:07:57.862678 [debug] [ThreadPool]: Using databricks connection "list_movies_default"
[0m02:07:57.863349 [debug] [ThreadPool]: On list_movies_default: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "connection_name": "list_movies_default"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'movies' 
  AND table_schema = 'default'

  
[0m02:07:57.863870 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:07:58.566717 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-81dd-131a-a453-e78cf94f49c5) - Created
[0m02:07:59.139034 [debug] [ThreadPool]: SQL status: OK in 1.280 seconds
[0m02:07:59.141907 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0cfe4-81dd-131a-a453-e78cf94f49c5, command-id=01f0cfe4-81f7-1805-9ffe-df50238e3514) - Closing
[0m02:07:59.143017 [debug] [ThreadPool]: On list_movies_default: Close
[0m02:07:59.143719 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0cfe4-81dd-131a-a453-e78cf94f49c5) - Closing
[0m02:07:59.360717 [debug] [Thread-1 (]: Began running node model.movies.default_credits
[0m02:07:59.361357 [debug] [Thread-2 (]: Began running node model.movies.default_keywords
[0m02:07:59.361926 [debug] [Thread-3 (]: Began running node model.movies.default_links
[0m02:07:59.362633 [debug] [Thread-4 (]: Began running node model.movies.default_links_small
[0m02:07:59.363331 [info ] [Thread-1 (]: 1 of 14 START sql view model default.default_credits ........................... [RUN]
[0m02:07:59.364130 [info ] [Thread-2 (]: 2 of 14 START sql view model default.default_keywords .......................... [RUN]
[0m02:07:59.364925 [info ] [Thread-3 (]: 3 of 14 START sql view model default.default_links ............................. [RUN]
[0m02:07:59.365687 [info ] [Thread-4 (]: 4 of 14 START sql view model default.default_links_small ....................... [RUN]
[0m02:07:59.366563 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_credits) - Creating connection
[0m02:07:59.367353 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_keywords) - Creating connection
[0m02:07:59.368172 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_links) - Creating connection
[0m02:07:59.368934 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_links_small) - Creating connection
[0m02:07:59.369658 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.movies.default_credits'
[0m02:07:59.370227 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.movies.default_keywords'
[0m02:07:59.370813 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.movies.default_links'
[0m02:07:59.371381 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.movies.default_links_small'
[0m02:07:59.372002 [debug] [Thread-1 (]: Began compiling node model.movies.default_credits
[0m02:07:59.372575 [debug] [Thread-2 (]: Began compiling node model.movies.default_keywords
[0m02:07:59.373143 [debug] [Thread-3 (]: Began compiling node model.movies.default_links
[0m02:07:59.373716 [debug] [Thread-4 (]: Began compiling node model.movies.default_links_small
[0m02:07:59.384604 [debug] [Thread-1 (]: Writing injected SQL for node "model.movies.default_credits"
[0m02:07:59.390347 [debug] [Thread-3 (]: Writing injected SQL for node "model.movies.default_links"
[0m02:07:59.401138 [debug] [Thread-4 (]: Writing injected SQL for node "model.movies.default_links_small"
[0m02:07:59.403715 [debug] [Thread-2 (]: Writing injected SQL for node "model.movies.default_keywords"
[0m02:07:59.405913 [debug] [Thread-3 (]: Began executing node model.movies.default_links
[0m02:07:59.412469 [debug] [Thread-1 (]: Began executing node model.movies.default_credits
[0m02:07:59.418490 [debug] [Thread-4 (]: Began executing node model.movies.default_links_small
[0m02:07:59.448452 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:07:59.453303 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:07:59.454023 [debug] [Thread-2 (]: Began executing node model.movies.default_keywords
[0m02:07:59.454631 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m02:07:59.457423 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:07:59.458377 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:07:59.462079 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m02:07:59.462955 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:07:59.480608 [debug] [Thread-3 (]: Creating view `movies`.`default`.`default_links`
[0m02:07:59.482112 [debug] [Thread-4 (]: Creating view `movies`.`default`.`default_links_small`
[0m02:07:59.483804 [debug] [Thread-2 (]: Creating view `movies`.`default`.`default_keywords`
[0m02:07:59.485398 [debug] [Thread-1 (]: Creating view `movies`.`default`.`default_credits`
[0m02:07:59.495049 [debug] [Thread-3 (]: Writing runtime sql for node "model.movies.default_links"
[0m02:07:59.496488 [debug] [Thread-4 (]: Writing runtime sql for node "model.movies.default_links_small"
[0m02:07:59.497790 [debug] [Thread-2 (]: Writing runtime sql for node "model.movies.default_keywords"
[0m02:07:59.499070 [debug] [Thread-1 (]: Writing runtime sql for node "model.movies.default_credits"
[0m02:07:59.501499 [debug] [Thread-1 (]: Using databricks connection "model.movies.default_credits"
[0m02:07:59.504603 [debug] [Thread-1 (]: On model.movies.default_credits: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_credits"} */

  
  
  create or replace view `movies`.`default`.`default_credits`
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  )

[0m02:07:59.505396 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:07:59.505978 [debug] [Thread-4 (]: Using databricks connection "model.movies.default_links_small"
[0m02:07:59.506474 [debug] [Thread-2 (]: Using databricks connection "model.movies.default_keywords"
[0m02:07:59.507274 [debug] [Thread-3 (]: Using databricks connection "model.movies.default_links"
[0m02:07:59.507839 [debug] [Thread-4 (]: On model.movies.default_links_small: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_links_small"} */

  
  
  create or replace view `movies`.`default`.`default_links_small`
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  )

[0m02:07:59.508722 [debug] [Thread-2 (]: On model.movies.default_keywords: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_keywords"} */

  
  
  create or replace view `movies`.`default`.`default_keywords`
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  )

[0m02:07:59.509551 [debug] [Thread-3 (]: On model.movies.default_links: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_links"} */

  
  
  create or replace view `movies`.`default`.`default_links`
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  )

[0m02:07:59.510523 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:07:59.511106 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m02:07:59.511764 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:08:00.332980 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-82e9-15ac-8b4f-a1f3981964a0) - Created
[0m02:08:00.338065 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-82e9-1f33-b496-3c409beb9ad3) - Created
[0m02:08:00.342565 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-82ea-18e4-9fd5-80da6bde93e2) - Created
[0m02:08:00.347858 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-82eb-1eab-840e-eafc9a9c83d0) - Created
[0m02:08:01.212200 [debug] [Thread-3 (]: SQL status: OK in 1.700 seconds
[0m02:08:01.213833 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0cfe4-82e9-1f33-b496-3c409beb9ad3, command-id=01f0cfe4-8305-16dc-8027-f094f97ab1cd) - Closing
[0m02:08:01.230309 [debug] [Thread-2 (]: SQL status: OK in 1.720 seconds
[0m02:08:01.232199 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:08:01.233383 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0cfe4-82e9-15ac-8b4f-a1f3981964a0, command-id=01f0cfe4-8304-1ba8-b5d3-f9f2c8128ba2) - Closing
[0m02:08:01.237558 [debug] [Thread-3 (]: On model.movies.default_links: Close
[0m02:08:01.238784 [debug] [Thread-2 (]: Applying tags to relation None
[0m02:08:01.239387 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-82e9-1f33-b496-3c409beb9ad3) - Closing
[0m02:08:01.292207 [debug] [Thread-4 (]: SQL status: OK in 1.780 seconds
[0m02:08:01.294042 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0cfe4-82eb-1eab-840e-eafc9a9c83d0, command-id=01f0cfe4-8307-13e0-acb5-bc18287772d4) - Closing
[0m02:08:01.295350 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:08:01.300776 [debug] [Thread-1 (]: SQL status: OK in 1.800 seconds
[0m02:08:01.302039 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0cfe4-82ea-18e4-9fd5-80da6bde93e2, command-id=01f0cfe4-8306-182e-882d-f0a773387b2a) - Closing
[0m02:08:01.303270 [debug] [Thread-1 (]: Applying tags to relation None
[0m02:08:01.432113 [debug] [Thread-2 (]: On model.movies.default_keywords: Close
[0m02:08:01.433169 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-82e9-15ac-8b4f-a1f3981964a0) - Closing
[0m02:08:01.619956 [debug] [Thread-4 (]: On model.movies.default_links_small: Close
[0m02:08:01.620773 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-82eb-1eab-840e-eafc9a9c83d0) - Closing
[0m02:08:01.823909 [debug] [Thread-1 (]: On model.movies.default_credits: Close
[0m02:08:01.824804 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-82ea-18e4-9fd5-80da6bde93e2) - Closing
[0m02:08:02.028549 [info ] [Thread-2 (]: 2 of 14 OK created sql view model default.default_keywords ..................... [[32mOK[0m in 2.66s]
[0m02:08:02.029267 [info ] [Thread-1 (]: 1 of 14 OK created sql view model default.default_credits ...................... [[32mOK[0m in 2.66s]
[0m02:08:02.031713 [debug] [Thread-2 (]: Finished running node model.movies.default_keywords
[0m02:08:02.029943 [info ] [Thread-3 (]: 3 of 14 OK created sql view model default.default_links ........................ [[32mOK[0m in 2.65s]
[0m02:08:02.030762 [info ] [Thread-4 (]: 4 of 14 OK created sql view model default.default_links_small .................. [[32mOK[0m in 2.66s]
[0m02:08:02.032927 [debug] [Thread-1 (]: Finished running node model.movies.default_credits
[0m02:08:02.033596 [debug] [Thread-2 (]: Began running node model.movies.default_movies_metadata
[0m02:08:02.034628 [debug] [Thread-3 (]: Finished running node model.movies.default_links
[0m02:08:02.035453 [debug] [Thread-4 (]: Finished running node model.movies.default_links_small
[0m02:08:02.036094 [debug] [Thread-1 (]: Began running node model.movies.default_ratings
[0m02:08:02.036921 [info ] [Thread-2 (]: 5 of 14 START sql view model default.default_movies_metadata ................... [RUN]
[0m02:08:02.037861 [debug] [Thread-3 (]: Began running node model.movies.default_ratings_small
[0m02:08:02.038634 [debug] [Thread-4 (]: Began running node model.movies.enh_default_credits
[0m02:08:02.039450 [info ] [Thread-1 (]: 6 of 14 START sql view model default.default_ratings ........................... [RUN]
[0m02:08:02.040374 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_movies_metadata) - Creating connection
[0m02:08:02.041120 [info ] [Thread-3 (]: 7 of 14 START sql view model default.default_ratings_small ..................... [RUN]
[0m02:08:02.041926 [info ] [Thread-4 (]: 8 of 14 START sql incremental model default.enh_default_credits ................ [RUN]
[0m02:08:02.042794 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_ratings) - Creating connection
[0m02:08:02.043397 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.movies.default_movies_metadata'
[0m02:08:02.044300 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.default_ratings_small) - Creating connection
[0m02:08:02.045090 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_credits) - Creating connection
[0m02:08:02.045692 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.movies.default_ratings'
[0m02:08:02.046305 [debug] [Thread-2 (]: Began compiling node model.movies.default_movies_metadata
[0m02:08:02.046884 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.movies.default_ratings_small'
[0m02:08:02.047461 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.movies.enh_default_credits'
[0m02:08:02.048066 [debug] [Thread-1 (]: Began compiling node model.movies.default_ratings
[0m02:08:02.053128 [debug] [Thread-2 (]: Writing injected SQL for node "model.movies.default_movies_metadata"
[0m02:08:02.053836 [debug] [Thread-3 (]: Began compiling node model.movies.default_ratings_small
[0m02:08:02.054464 [debug] [Thread-4 (]: Began compiling node model.movies.enh_default_credits
[0m02:08:02.064096 [debug] [Thread-3 (]: Writing injected SQL for node "model.movies.default_ratings_small"
[0m02:08:02.072013 [debug] [Thread-4 (]: Writing injected SQL for node "model.movies.enh_default_credits"
[0m02:08:02.077930 [debug] [Thread-2 (]: Began executing node model.movies.default_movies_metadata
[0m02:08:02.081252 [debug] [Thread-1 (]: Writing injected SQL for node "model.movies.default_ratings"
[0m02:08:02.086088 [debug] [Thread-2 (]: MATERIALIZING VIEW
[0m02:08:02.088507 [debug] [Thread-2 (]: Creating view `movies`.`default`.`default_movies_metadata`
[0m02:08:02.089134 [debug] [Thread-3 (]: Began executing node model.movies.default_ratings_small
[0m02:08:02.090622 [debug] [Thread-2 (]: Writing runtime sql for node "model.movies.default_movies_metadata"
[0m02:08:02.094649 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:08:02.095320 [debug] [Thread-4 (]: Began executing node model.movies.enh_default_credits
[0m02:08:02.095907 [debug] [Thread-1 (]: Began executing node model.movies.default_ratings
[0m02:08:02.098532 [debug] [Thread-3 (]: Creating view `movies`.`default`.`default_ratings_small`
[0m02:08:02.123339 [debug] [Thread-2 (]: Using databricks connection "model.movies.default_movies_metadata"
[0m02:08:02.145334 [debug] [Thread-1 (]: MATERIALIZING VIEW
[0m02:08:02.154147 [debug] [Thread-4 (]: MATERIALIZING INCREMENTAL
[0m02:08:02.155745 [debug] [Thread-3 (]: Writing runtime sql for node "model.movies.default_ratings_small"
[0m02:08:02.156441 [debug] [Thread-2 (]: On model.movies.default_movies_metadata: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_movies_metadata"} */

  
  
  create or replace view `movies`.`default`.`default_movies_metadata`
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  )

[0m02:08:02.158308 [debug] [Thread-1 (]: Creating view `movies`.`default`.`default_ratings`
[0m02:08:02.217007 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m02:08:02.227434 [debug] [Thread-4 (]: Writing runtime sql for node "model.movies.enh_default_credits"
[0m02:08:02.228978 [debug] [Thread-1 (]: Writing runtime sql for node "model.movies.default_ratings"
[0m02:08:02.229511 [debug] [Thread-3 (]: Using databricks connection "model.movies.default_ratings_small"
[0m02:08:02.230833 [debug] [Thread-3 (]: On model.movies.default_ratings_small: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_ratings_small"} */

  
  
  create or replace view `movies`.`default`.`default_ratings_small`
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  )

[0m02:08:02.231685 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:08:02.232532 [debug] [Thread-4 (]: Using databricks connection "model.movies.enh_default_credits"
[0m02:08:02.233140 [debug] [Thread-1 (]: Using databricks connection "model.movies.default_ratings"
[0m02:08:02.233950 [debug] [Thread-4 (]: On model.movies.enh_default_credits: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */

  
    
        create or replace table `movies`.`default`.`enh_default_credits`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_credits`
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  


)
  
[0m02:08:02.234650 [debug] [Thread-1 (]: On model.movies.default_ratings: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.default_ratings"} */

  
  
  create or replace view `movies`.`default`.`default_ratings`
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  )

[0m02:08:02.235213 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:08:02.235747 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:08:03.083527 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-848b-1ef5-95bc-2157fc38b38b) - Created
[0m02:08:03.085138 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-848c-1dbb-942f-1f2278178bd1) - Created
[0m02:08:03.099672 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-8490-12e6-a5bc-0c8198296227) - Created
[0m02:08:03.104166 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-8490-14e4-bc0d-245c343a371e) - Created
[0m02:08:03.559400 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */

  
    
        create or replace table `movies`.`default`.`enh_default_credits`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_credits`
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */

  
    
        create or replace table `movies`.`default`.`enh_default_credits`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_credits`
------^^^
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */

  
    
        create or replace table `movies`.`default`.`enh_default_credits`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_credits`
------^^^
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */

  
    
        create or replace table `movies`.`default`.`enh_default_credits`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_credits`
------^^^
  
  as (
    with credits as (
	select
		cast,
		crew,
		id
	from `movies`.`default`.`credits`
)

select * from credits
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-84aa-1ff2-b293-34dfac705e09
[0m02:08:03.561439 [debug] [Thread-4 (]: On model.movies.enh_default_credits: Close
[0m02:08:03.562096 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-8490-12e6-a5bc-0c8198296227) - Closing
[0m02:08:03.839200 [debug] [Thread-3 (]: SQL status: OK in 1.610 seconds
[0m02:08:03.840659 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0cfe4-848c-1dbb-942f-1f2278178bd1, command-id=01f0cfe4-84a8-1736-8d7c-73575b616670) - Closing
[0m02:08:03.841912 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:08:03.843160 [debug] [Thread-4 (]: Database Error in model enh_default_credits (models/enhanced/enh_default_credits.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_credits`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_credits`
  ------^^^
    
    as (
      with credits as (
  	select
  		cast,
  		crew,
  		id
  	from `movies`.`default`.`credits`
  )
  
  select * from credits
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_credits.sql
[0m02:08:03.844482 [debug] [Thread-3 (]: On model.movies.default_ratings_small: Close
[0m02:08:03.845372 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-848c-1dbb-942f-1f2278178bd1) - Closing
[0m02:08:03.885248 [debug] [Thread-1 (]: SQL status: OK in 1.650 seconds
[0m02:08:03.886990 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f0cfe4-8490-14e4-bc0d-245c343a371e, command-id=01f0cfe4-84ab-1eaa-ad1a-2b29ff2bee71) - Closing
[0m02:08:03.888301 [debug] [Thread-1 (]: Applying tags to relation None
[0m02:08:03.895751 [debug] [Thread-2 (]: SQL status: OK in 1.680 seconds
[0m02:08:03.897034 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=01f0cfe4-848b-1ef5-95bc-2157fc38b38b, command-id=01f0cfe4-84a7-1f11-8e0f-136d9766ea4c) - Closing
[0m02:08:03.898175 [debug] [Thread-2 (]: Applying tags to relation None
[0m02:08:04.040197 [debug] [Thread-1 (]: On model.movies.default_ratings: Close
[0m02:08:04.039560 [error] [Thread-4 (]: 8 of 14 ERROR creating sql incremental model default.enh_default_credits ....... [[31mERROR[0m in 1.99s]
[0m02:08:04.041138 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-8490-14e4-bc0d-245c343a371e) - Closing
[0m02:08:04.042005 [debug] [Thread-4 (]: Finished running node model.movies.enh_default_credits
[0m02:08:04.043198 [debug] [Thread-4 (]: Began running node model.movies.enh_default_keywords
[0m02:08:04.044032 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_credits' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_credits (models/enhanced/enh_default_credits.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_credits`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_credits`
  ------^^^
    
    as (
      with credits as (
  	select
  		cast,
  		crew,
  		id
  	from `movies`.`default`.`credits`
  )
  
  select * from credits
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_credits.sql.
[0m02:08:04.045085 [info ] [Thread-4 (]: 9 of 14 START sql incremental model default.enh_default_keywords ............... [RUN]
[0m02:08:04.276640 [debug] [Thread-2 (]: On model.movies.default_movies_metadata: Close
[0m02:08:04.277899 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-848b-1ef5-95bc-2157fc38b38b) - Closing
[0m02:08:04.472173 [info ] [Thread-3 (]: 7 of 14 OK created sql view model default.default_ratings_small ................ [[32mOK[0m in 2.43s]
[0m02:08:04.472917 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_keywords) - Creating connection
[0m02:08:04.473841 [info ] [Thread-1 (]: 6 of 14 OK created sql view model default.default_ratings ...................... [[32mOK[0m in 2.43s]
[0m02:08:04.474740 [info ] [Thread-2 (]: 5 of 14 OK created sql view model default.default_movies_metadata .............. [[32mOK[0m in 2.43s]
[0m02:08:04.475646 [debug] [Thread-3 (]: Finished running node model.movies.default_ratings_small
[0m02:08:04.476245 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.movies.enh_default_keywords'
[0m02:08:04.477307 [debug] [Thread-1 (]: Finished running node model.movies.default_ratings
[0m02:08:04.478135 [debug] [Thread-2 (]: Finished running node model.movies.default_movies_metadata
[0m02:08:04.478777 [debug] [Thread-3 (]: Began running node model.movies.enh_default_links
[0m02:08:04.479442 [debug] [Thread-4 (]: Began compiling node model.movies.enh_default_keywords
[0m02:08:04.480085 [debug] [Thread-1 (]: Began running node model.movies.enh_default_links_small
[0m02:08:04.480795 [debug] [Thread-2 (]: Began running node model.movies.enh_default_movies_metadata
[0m02:08:04.481611 [info ] [Thread-3 (]: 10 of 14 START sql incremental model default.enh_default_links ................. [RUN]
[0m02:08:04.487401 [debug] [Thread-4 (]: Writing injected SQL for node "model.movies.enh_default_keywords"
[0m02:08:04.488267 [info ] [Thread-1 (]: 11 of 14 START sql incremental model default.enh_default_links_small ........... [RUN]
[0m02:08:04.489051 [info ] [Thread-2 (]: 12 of 14 START sql incremental model default.enh_default_movies_metadata ....... [RUN]
[0m02:08:04.489926 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_links) - Creating connection
[0m02:08:04.490825 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_links_small) - Creating connection
[0m02:08:04.491618 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_movies_metadata) - Creating connection
[0m02:08:04.492293 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.movies.enh_default_links'
[0m02:08:04.492836 [debug] [Thread-4 (]: Began executing node model.movies.enh_default_keywords
[0m02:08:04.493411 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.movies.enh_default_links_small'
[0m02:08:04.494008 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.movies.enh_default_movies_metadata'
[0m02:08:04.494629 [debug] [Thread-3 (]: Began compiling node model.movies.enh_default_links
[0m02:08:04.498271 [debug] [Thread-4 (]: MATERIALIZING INCREMENTAL
[0m02:08:04.498904 [debug] [Thread-1 (]: Began compiling node model.movies.enh_default_links_small
[0m02:08:04.499492 [debug] [Thread-2 (]: Began compiling node model.movies.enh_default_movies_metadata
[0m02:08:04.506220 [debug] [Thread-3 (]: Writing injected SQL for node "model.movies.enh_default_links"
[0m02:08:04.514023 [debug] [Thread-1 (]: Writing injected SQL for node "model.movies.enh_default_links_small"
[0m02:08:04.521434 [debug] [Thread-2 (]: Writing injected SQL for node "model.movies.enh_default_movies_metadata"
[0m02:08:04.530491 [debug] [Thread-4 (]: Writing runtime sql for node "model.movies.enh_default_keywords"
[0m02:08:04.532515 [debug] [Thread-3 (]: Began executing node model.movies.enh_default_links
[0m02:08:04.537115 [debug] [Thread-3 (]: MATERIALIZING INCREMENTAL
[0m02:08:04.537764 [debug] [Thread-4 (]: Using databricks connection "model.movies.enh_default_keywords"
[0m02:08:04.538433 [debug] [Thread-2 (]: Began executing node model.movies.enh_default_movies_metadata
[0m02:08:04.538982 [debug] [Thread-1 (]: Began executing node model.movies.enh_default_links_small
[0m02:08:04.543090 [debug] [Thread-3 (]: Writing runtime sql for node "model.movies.enh_default_links"
[0m02:08:04.543844 [debug] [Thread-4 (]: On model.movies.enh_default_keywords: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */

  
    
        create or replace table `movies`.`default`.`enh_default_keywords`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_keywords`
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  


)
  
[0m02:08:04.547743 [debug] [Thread-2 (]: MATERIALIZING INCREMENTAL
[0m02:08:04.551405 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m02:08:04.552519 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:08:04.557192 [debug] [Thread-2 (]: Writing runtime sql for node "model.movies.enh_default_movies_metadata"
[0m02:08:04.561543 [debug] [Thread-1 (]: Writing runtime sql for node "model.movies.enh_default_links_small"
[0m02:08:04.562130 [debug] [Thread-3 (]: Using databricks connection "model.movies.enh_default_links"
[0m02:08:04.563587 [debug] [Thread-3 (]: On model.movies.enh_default_links: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links`
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  


)
  
[0m02:08:04.564474 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:08:04.565435 [debug] [Thread-2 (]: Using databricks connection "model.movies.enh_default_movies_metadata"
[0m02:08:04.566111 [debug] [Thread-1 (]: Using databricks connection "model.movies.enh_default_links_small"
[0m02:08:04.566940 [debug] [Thread-2 (]: On model.movies.enh_default_movies_metadata: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */

  
    
        create or replace table `movies`.`default`.`enh_default_movies_metadata`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_movies_metadata`
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  


)
  
[0m02:08:04.567679 [debug] [Thread-1 (]: On model.movies.enh_default_links_small: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links_small`
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  


)
  
[0m02:08:04.568407 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m02:08:04.568981 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:08:05.373257 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-85e9-1c1e-ad99-48819cbebef8) - Created
[0m02:08:05.395407 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-85ed-1506-b58f-ddda7355ded9) - Created
[0m02:08:05.454088 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-85f7-11b6-b32e-a8f579ae2d71) - Created
[0m02:08:05.456092 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-85f6-1bd5-ad16-be7d3a29b3d4) - Created
[0m02:08:05.751074 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */

  
    
        create or replace table `movies`.`default`.`enh_default_keywords`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_keywords`
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */

  
    
        create or replace table `movies`.`default`.`enh_default_keywords`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_keywords`
------^^^
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */

  
    
        create or replace table `movies`.`default`.`enh_default_keywords`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_keywords`
------^^^
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */

  
    
        create or replace table `movies`.`default`.`enh_default_keywords`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_keywords`
------^^^
  
  as (
    with keywords as (
	select
		id,
		keywords
	from `movies`.`default`.`keywords`
)

select * from keywords
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-8606-1ca0-be8b-a50396ca2a66
[0m02:08:05.753165 [debug] [Thread-4 (]: On model.movies.enh_default_keywords: Close
[0m02:08:05.754134 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-85e9-1c1e-ad99-48819cbebef8) - Closing
[0m02:08:05.775063 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links`
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links`
------^^^
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links`
------^^^
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links`
------^^^
  
  as (
    with links as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links`
)

select * from links
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-8609-155b-886c-9615376e9ec9
[0m02:08:05.864664 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */

  
    
        create or replace table `movies`.`default`.`enh_default_movies_metadata`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_movies_metadata`
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */

  
    
        create or replace table `movies`.`default`.`enh_default_movies_metadata`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_movies_metadata`
------^^^
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */

  
    
        create or replace table `movies`.`default`.`enh_default_movies_metadata`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_movies_metadata`
------^^^
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */

  
    
        create or replace table `movies`.`default`.`enh_default_movies_metadata`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_movies_metadata`
------^^^
  
  as (
    with movies_metadata as (
	select
		adult,
		belongs_to_collection,
		budget,
		genres,
		homepage,
		id,
		imdb_id,
		original_language,
		original_title,
		overview,
		popularity,
		poster_path,
		production_companies,
		production_countries,
		release_date,
		revenue,
		runtime,
		spoken_languages,
		status,
		tagline,
		title,
		video,
		vote_average,
		vote_count
	from `movies`.`default`.`movies_metadata`
)

select * from movies_metadata
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-8612-1b61-90c7-66d6816cc73b
[0m02:08:05.873796 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links_small`
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links_small`
------^^^
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links_small`
------^^^
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_links_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_links_small`
------^^^
  
  as (
    with links_small as (
	select
		movieId,
		imdbId,
		tmdbId
	from `movies`.`default`.`links_small`
)

select * from links_small
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-8613-1644-9132-4699d58127e7
[0m02:08:05.953993 [debug] [Thread-3 (]: On model.movies.enh_default_links: Close
[0m02:08:05.954899 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-85ed-1506-b58f-ddda7355ded9) - Closing
[0m02:08:05.956759 [debug] [Thread-4 (]: Database Error in model enh_default_keywords (models/enhanced/enh_default_keywords.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_keywords`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_keywords`
  ------^^^
    
    as (
      with keywords as (
  	select
  		id,
  		keywords
  	from `movies`.`default`.`keywords`
  )
  
  select * from keywords
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_keywords.sql
[0m02:08:06.152783 [debug] [Thread-2 (]: On model.movies.enh_default_movies_metadata: Close
[0m02:08:06.153926 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0cfe4-85f7-11b6-b32e-a8f579ae2d71) - Closing
[0m02:08:06.155890 [debug] [Thread-3 (]: Database Error in model enh_default_links (models/enhanced/enh_default_links.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_links`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_links`
  ------^^^
    
    as (
      with links as (
  	select
  		movieId,
  		imdbId,
  		tmdbId
  	from `movies`.`default`.`links`
  )
  
  select * from links
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_links.sql
[0m02:08:06.345382 [debug] [Thread-1 (]: On model.movies.enh_default_links_small: Close
[0m02:08:06.346138 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f0cfe4-85f6-1bd5-ad16-be7d3a29b3d4) - Closing
[0m02:08:06.347985 [debug] [Thread-2 (]: Database Error in model enh_default_movies_metadata (models/enhanced/enh_default_movies_metadata.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_movies_metadata`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_movies_metadata`
  ------^^^
    
    as (
      with movies_metadata as (
  	select
  		adult,
  		belongs_to_collection,
  		budget,
  		genres,
  		homepage,
  		id,
  		imdb_id,
  		original_language,
  		original_title,
  		overview,
  		popularity,
  		poster_path,
  		production_companies,
  		production_countries,
  		release_date,
  		revenue,
  		runtime,
  		spoken_languages,
  		status,
  		tagline,
  		title,
  		video,
  		vote_average,
  		vote_count
  	from `movies`.`default`.`movies_metadata`
  )
  
  select * from movies_metadata
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_movies_metadata.sql
[0m02:08:06.533662 [error] [Thread-4 (]: 9 of 14 ERROR creating sql incremental model default.enh_default_keywords ...... [[31mERROR[0m in 2.49s]
[0m02:08:06.535041 [debug] [Thread-4 (]: Finished running node model.movies.enh_default_keywords
[0m02:08:06.536174 [error] [Thread-3 (]: 10 of 14 ERROR creating sql incremental model default.enh_default_links ........ [[31mERROR[0m in 2.05s]
[0m02:08:06.537543 [error] [Thread-2 (]: 12 of 14 ERROR creating sql incremental model default.enh_default_movies_metadata  [[31mERROR[0m in 2.05s]
[0m02:08:06.538330 [debug] [Thread-4 (]: Began running node model.movies.enh_default_ratings
[0m02:08:06.539620 [debug] [Thread-1 (]: Database Error in model enh_default_links_small (models/enhanced/enh_default_links_small.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_links_small`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_links_small`
  ------^^^
    
    as (
      with links_small as (
  	select
  		movieId,
  		imdbId,
  		tmdbId
  	from `movies`.`default`.`links_small`
  )
  
  select * from links_small
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_links_small.sql
[0m02:08:06.540528 [debug] [Thread-3 (]: Finished running node model.movies.enh_default_links
[0m02:08:06.541235 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_keywords' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_keywords (models/enhanced/enh_default_keywords.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_keywords`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_keywords`
  ------^^^
    
    as (
      with keywords as (
  	select
  		id,
  		keywords
  	from `movies`.`default`.`keywords`
  )
  
  select * from keywords
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_keywords.sql.
[0m02:08:06.542132 [debug] [Thread-2 (]: Finished running node model.movies.enh_default_movies_metadata
[0m02:08:06.542874 [info ] [Thread-4 (]: 13 of 14 START sql incremental model default.enh_default_ratings ............... [RUN]
[0m02:08:06.543950 [error] [Thread-1 (]: 11 of 14 ERROR creating sql incremental model default.enh_default_links_small .. [[31mERROR[0m in 2.05s]
[0m02:08:06.544901 [debug] [Thread-3 (]: Began running node model.movies.enh_default_ratings_small
[0m02:08:06.545800 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_links' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_links (models/enhanced/enh_default_links.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_links`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_links`
  ------^^^
    
    as (
      with links as (
  	select
  		movieId,
  		imdbId,
  		tmdbId
  	from `movies`.`default`.`links`
  )
  
  select * from links
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_links.sql.
[0m02:08:06.546751 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_ratings) - Creating connection
[0m02:08:06.547611 [debug] [Thread-1 (]: Finished running node model.movies.enh_default_links_small
[0m02:08:06.548336 [info ] [Thread-3 (]: 14 of 14 START sql incremental model default.enh_default_ratings_small ......... [RUN]
[0m02:08:06.549290 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_movies_metadata' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_movies_metadata (models/enhanced/enh_default_movies_metadata.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_movies_metadata`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_movies_metadata`
  ------^^^
    
    as (
      with movies_metadata as (
  	select
  		adult,
  		belongs_to_collection,
  		budget,
  		genres,
  		homepage,
  		id,
  		imdb_id,
  		original_language,
  		original_title,
  		overview,
  		popularity,
  		poster_path,
  		production_companies,
  		production_countries,
  		release_date,
  		revenue,
  		runtime,
  		spoken_languages,
  		status,
  		tagline,
  		title,
  		video,
  		vote_average,
  		vote_count
  	from `movies`.`default`.`movies_metadata`
  )
  
  select * from movies_metadata
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_movies_metadata.sql.
[0m02:08:06.549922 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.movies.enh_default_ratings'
[0m02:08:06.550801 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.movies.enh_default_ratings_small) - Creating connection
[0m02:08:06.551757 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_links_small' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_links_small (models/enhanced/enh_default_links_small.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_links_small`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_links_small`
  ------^^^
    
    as (
      with links_small as (
  	select
  		movieId,
  		imdbId,
  		tmdbId
  	from `movies`.`default`.`links_small`
  )
  
  select * from links_small
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_links_small.sql.
[0m02:08:06.552413 [debug] [Thread-4 (]: Began compiling node model.movies.enh_default_ratings
[0m02:08:06.553182 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.movies.enh_default_ratings_small'
[0m02:08:06.557968 [debug] [Thread-3 (]: Began compiling node model.movies.enh_default_ratings_small
[0m02:08:06.564967 [debug] [Thread-3 (]: Writing injected SQL for node "model.movies.enh_default_ratings_small"
[0m02:08:06.573954 [debug] [Thread-4 (]: Writing injected SQL for node "model.movies.enh_default_ratings"
[0m02:08:06.575765 [debug] [Thread-4 (]: Began executing node model.movies.enh_default_ratings
[0m02:08:06.576365 [debug] [Thread-3 (]: Began executing node model.movies.enh_default_ratings_small
[0m02:08:06.580350 [debug] [Thread-4 (]: MATERIALIZING INCREMENTAL
[0m02:08:06.583764 [debug] [Thread-3 (]: MATERIALIZING INCREMENTAL
[0m02:08:06.588158 [debug] [Thread-4 (]: Writing runtime sql for node "model.movies.enh_default_ratings"
[0m02:08:06.593650 [debug] [Thread-3 (]: Writing runtime sql for node "model.movies.enh_default_ratings_small"
[0m02:08:06.595714 [debug] [Thread-4 (]: Using databricks connection "model.movies.enh_default_ratings"
[0m02:08:06.596501 [debug] [Thread-4 (]: On model.movies.enh_default_ratings: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings`
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  


)
  
[0m02:08:06.597122 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:08:06.597600 [debug] [Thread-3 (]: Using databricks connection "model.movies.enh_default_ratings_small"
[0m02:08:06.598535 [debug] [Thread-3 (]: On model.movies.enh_default_ratings_small: /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings_small`
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  


)
  
[0m02:08:06.599268 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:08:07.347198 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-8717-103f-9a79-84d1571fd7da) - Created
[0m02:08:07.349984 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-8716-1a52-b8ec-f587e23e2fa8) - Created
[0m02:08:07.723928 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings_small`
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings_small`
------^^^
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings_small`
------^^^
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings_small`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings_small`
------^^^
  
  as (
    with ratings_small as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings_small`
)

select * from ratings_small
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-8733-1a94-96cc-2bf39081e88c
[0m02:08:07.725710 [debug] [Thread-3 (]: On model.movies.enh_default_ratings_small: Close
[0m02:08:07.726410 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0cfe4-8717-103f-9a79-84d1571fd7da) - Closing
[0m02:08:07.739997 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings`
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  


)
  
: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings`
------^^^
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  


)

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings`
------^^^
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  


)

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1040)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:778)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:853)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:569)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:532)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */

  
    
        create or replace table `movies`.`default`.`enh_default_ratings`
      
      
  using delta
      
      
      
      
      
      
      
      as
      create or replace view `movies`.`default`.`default_ratings`
------^^^
  
  as (
    with ratings as (
	select
		userId,
		movieId,
		rating,
		timestamp
	from `movies`.`default`.`ratings`
)

select * from ratings
  


)

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:478)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:119)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:163)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:697)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:158)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:139)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:886)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:881)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more
, operation-id=01f0cfe4-8734-126a-953b-3c6d5a1666ce
[0m02:08:07.925619 [debug] [Thread-4 (]: On model.movies.enh_default_ratings: Close
[0m02:08:07.926442 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0cfe4-8716-1a52-b8ec-f587e23e2fa8) - Closing
[0m02:08:07.928519 [debug] [Thread-3 (]: Database Error in model enh_default_ratings_small (models/enhanced/enh_default_ratings_small.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_ratings_small`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_ratings_small`
  ------^^^
    
    as (
      with ratings_small as (
  	select
  		userId,
  		movieId,
  		rating,
  		timestamp
  	from `movies`.`default`.`ratings_small`
  )
  
  select * from ratings_small
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_ratings_small.sql
[0m02:08:08.150151 [error] [Thread-3 (]: 14 of 14 ERROR creating sql incremental model default.enh_default_ratings_small  [[31mERROR[0m in 1.60s]
[0m02:08:08.151244 [debug] [Thread-3 (]: Finished running node model.movies.enh_default_ratings_small
[0m02:08:08.152199 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_ratings_small' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_ratings_small (models/enhanced/enh_default_ratings_small.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_ratings_small`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_ratings_small`
  ------^^^
    
    as (
      with ratings_small as (
  	select
  		userId,
  		movieId,
  		rating,
  		timestamp
  	from `movies`.`default`.`ratings_small`
  )
  
  select * from ratings_small
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_ratings_small.sql.
[0m02:08:08.153903 [debug] [Thread-4 (]: Database Error in model enh_default_ratings (models/enhanced/enh_default_ratings.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_ratings`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_ratings`
  ------^^^
    
    as (
      with ratings as (
  	select
  		userId,
  		movieId,
  		rating,
  		timestamp
  	from `movies`.`default`.`ratings`
  )
  
  select * from ratings
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_ratings.sql
[0m02:08:08.155006 [error] [Thread-4 (]: 13 of 14 ERROR creating sql incremental model default.enh_default_ratings ...... [[31mERROR[0m in 1.61s]
[0m02:08:08.155902 [debug] [Thread-4 (]: Finished running node model.movies.enh_default_ratings
[0m02:08:08.156709 [debug] [Thread-7 (]: Marking all children of 'model.movies.enh_default_ratings' to be skipped because of status 'error'.  Reason: Database Error in model enh_default_ratings (models/enhanced/enh_default_ratings.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_ratings`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_ratings`
  ------^^^
    
    as (
      with ratings as (
  	select
  		userId,
  		movieId,
  		rating,
  		timestamp
  	from `movies`.`default`.`ratings`
  )
  
  select * from ratings
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_ratings.sql.
[0m02:08:08.158264 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:08:08.158793 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:08:08.159536 [info ] [MainThread]: 
[0m02:08:08.160080 [info ] [MainThread]: Finished running 7 incremental models, 7 view models in 0 hours 0 minutes and 11.76 seconds (11.76s).
[0m02:08:08.163922 [debug] [MainThread]: Command end result
[0m02:08:08.277897 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/manifest.json
[0m02:08:08.283090 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nuri/rosetta-dbt-studio-projects/movies/target/semantic_manifest.json
[0m02:08:08.329997 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nuri/rosetta-dbt-studio-projects/movies/target/run_results.json
[0m02:08:08.330563 [info ] [MainThread]: 
[0m02:08:08.331135 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m02:08:08.331642 [info ] [MainThread]: 
[0m02:08:08.332229 [error] [MainThread]: [31mFailure in model enh_default_credits (models/enhanced/enh_default_credits.sql)[0m
[0m02:08:08.332869 [error] [MainThread]:   Database Error in model enh_default_credits (models/enhanced/enh_default_credits.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_credits"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_credits`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_credits`
  ------^^^
    
    as (
      with credits as (
  	select
  		cast,
  		crew,
  		id
  	from `movies`.`default`.`credits`
  )
  
  select * from credits
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_credits.sql
[0m02:08:08.333412 [info ] [MainThread]: 
[0m02:08:08.333971 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_credits.sql
[0m02:08:08.334465 [info ] [MainThread]: 
[0m02:08:08.335372 [error] [MainThread]: [31mFailure in model enh_default_keywords (models/enhanced/enh_default_keywords.sql)[0m
[0m02:08:08.336423 [error] [MainThread]:   Database Error in model enh_default_keywords (models/enhanced/enh_default_keywords.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_keywords"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_keywords`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_keywords`
  ------^^^
    
    as (
      with keywords as (
  	select
  		id,
  		keywords
  	from `movies`.`default`.`keywords`
  )
  
  select * from keywords
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_keywords.sql
[0m02:08:08.337299 [info ] [MainThread]: 
[0m02:08:08.338295 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_keywords.sql
[0m02:08:08.338875 [info ] [MainThread]: 
[0m02:08:08.340010 [error] [MainThread]: [31mFailure in model enh_default_links (models/enhanced/enh_default_links.sql)[0m
[0m02:08:08.346127 [error] [MainThread]:   Database Error in model enh_default_links (models/enhanced/enh_default_links.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_links`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_links`
  ------^^^
    
    as (
      with links as (
  	select
  		movieId,
  		imdbId,
  		tmdbId
  	from `movies`.`default`.`links`
  )
  
  select * from links
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_links.sql
[0m02:08:08.346743 [info ] [MainThread]: 
[0m02:08:08.347328 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_links.sql
[0m02:08:08.347839 [info ] [MainThread]: 
[0m02:08:08.348428 [error] [MainThread]: [31mFailure in model enh_default_movies_metadata (models/enhanced/enh_default_movies_metadata.sql)[0m
[0m02:08:08.349073 [error] [MainThread]:   Database Error in model enh_default_movies_metadata (models/enhanced/enh_default_movies_metadata.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_movies_metadata"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_movies_metadata`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_movies_metadata`
  ------^^^
    
    as (
      with movies_metadata as (
  	select
  		adult,
  		belongs_to_collection,
  		budget,
  		genres,
  		homepage,
  		id,
  		imdb_id,
  		original_language,
  		original_title,
  		overview,
  		popularity,
  		poster_path,
  		production_companies,
  		production_countries,
  		release_date,
  		revenue,
  		runtime,
  		spoken_languages,
  		status,
  		tagline,
  		title,
  		video,
  		vote_average,
  		vote_count
  	from `movies`.`default`.`movies_metadata`
  )
  
  select * from movies_metadata
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_movies_metadata.sql
[0m02:08:08.349627 [info ] [MainThread]: 
[0m02:08:08.350195 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_movies_metadata.sql
[0m02:08:08.350702 [info ] [MainThread]: 
[0m02:08:08.351307 [error] [MainThread]: [31mFailure in model enh_default_links_small (models/enhanced/enh_default_links_small.sql)[0m
[0m02:08:08.352017 [error] [MainThread]:   Database Error in model enh_default_links_small (models/enhanced/enh_default_links_small.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_links_small"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_links_small`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_links_small`
  ------^^^
    
    as (
      with links_small as (
  	select
  		movieId,
  		imdbId,
  		tmdbId
  	from `movies`.`default`.`links_small`
  )
  
  select * from links_small
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_links_small.sql
[0m02:08:08.352563 [info ] [MainThread]: 
[0m02:08:08.353250 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_links_small.sql
[0m02:08:08.353764 [info ] [MainThread]: 
[0m02:08:08.354350 [error] [MainThread]: [31mFailure in model enh_default_ratings_small (models/enhanced/enh_default_ratings_small.sql)[0m
[0m02:08:08.354985 [error] [MainThread]:   Database Error in model enh_default_ratings_small (models/enhanced/enh_default_ratings_small.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings_small"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_ratings_small`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_ratings_small`
  ------^^^
    
    as (
      with ratings_small as (
  	select
  		userId,
  		movieId,
  		rating,
  		timestamp
  	from `movies`.`default`.`ratings_small`
  )
  
  select * from ratings_small
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_ratings_small.sql
[0m02:08:08.355525 [info ] [MainThread]: 
[0m02:08:08.356086 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_ratings_small.sql
[0m02:08:08.356585 [info ] [MainThread]: 
[0m02:08:08.357160 [error] [MainThread]: [31mFailure in model enh_default_ratings (models/enhanced/enh_default_ratings.sql)[0m
[0m02:08:08.357765 [error] [MainThread]:   Database Error in model enh_default_ratings (models/enhanced/enh_default_ratings.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create'. SQLSTATE: 42601 (line 17, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.10.11", "dbt_databricks_version": "1.10.12", "databricks_sql_connector_version": "4.0.5", "profile_name": "movies", "target_name": "dev", "node_id": "model.movies.enh_default_ratings"} */
  
    
      
          create or replace table `movies`.`default`.`enh_default_ratings`
        
        
    using delta
        
        
        
        
        
        
        
        as
        create or replace view `movies`.`default`.`default_ratings`
  ------^^^
    
    as (
      with ratings as (
  	select
  		userId,
  		movieId,
  		rating,
  		timestamp
  	from `movies`.`default`.`ratings`
  )
  
  select * from ratings
    
  
  
  )
  
  compiled code at target/run/movies/models/enhanced/enh_default_ratings.sql
[0m02:08:08.358284 [info ] [MainThread]: 
[0m02:08:08.358858 [info ] [MainThread]:   compiled code at target/compiled/movies/models/enhanced/enh_default_ratings.sql
[0m02:08:08.359461 [info ] [MainThread]: 
[0m02:08:08.360021 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=7 SKIP=0 NO-OP=0 TOTAL=14
[0m02:08:08.360861 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ProjectFlagsMovedDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:08:08.362043 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 16.45969, "process_in_blocks": "0", "process_kernel_time": 0.954694, "process_mem_max_rss": "223457280", "process_out_blocks": "0", "process_user_time": 7.669449}
[0m02:08:08.362727 [debug] [MainThread]: Command `dbt run` failed at 02:08:08.362589 after 16.46 seconds
[0m02:08:08.363306 [debug] [MainThread]: Flushing usage events
